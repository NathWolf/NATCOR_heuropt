{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathWolf/NATCOR_heuropt/blob/main/NATCOR_Challenge_4_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark MLlib Tutorial and Challenge"
      ],
      "metadata": {
        "id": "oe2Z4Hgzb4LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on the resources available alongside the book:\n",
        "\n",
        "> Triguero, I., & Galar, M. (2023). **Large-Scale Data Analytics with Python and Spark: A Hands-on Guide to Implementing Machine Learning Solutions.** Cambridge: Cambridge University Press"
      ],
      "metadata": {
        "id": "T1Jidp0DcNgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this notebook is to play with the MLlib of Apache Spark to create a Machine Learning pipeline that preprocesses a dataset, trains a model and makes predictions. In particular, we are going to deal with a regression problem for energy consumption prediction. The content is very much inspired by the sample provided in the [databricks documentation](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2854662143668609/2084788691983918/6837869239396014/latest.html). Throughout this notebook you will have to complete all the steps to learn from the data, ranging from loading the data and preprocessing it to learn a model and evaluate the results. To do this, we will use MLlib pipelines.\n",
        "\n",
        "The first part of the notebook provides a Guided Tutorial, building on the MLlib example you saw in the lecture to build a pipeline and begin to investigate the data and results. There are several small tasks you will need to fill in the code for. Some parts you can complete based on the lecture example, but for others you will need to investigate the Spark SQL and MLlib documentation yourself!\n",
        "\n",
        "The second part of the notebook provides some initial ideas for improving the results further. The challenge for you is to explore some options and see if you can improve on the model from the tutorial. This may require doing some brief study of machine learning concepts if you are unfamiliar with this area, and further exploring the functions available to you in the Spark MLlib."
      ],
      "metadata": {
        "id": "kPPicN8Zb-WB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>**If you are using this notebook in Colab, don't forget to save a copy to your own Google Drive! Otherwise you will not be able to save any changes that you make.**</font>"
      ],
      "metadata": {
        "id": "6u5h647wqd0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Guided Tutorial"
      ],
      "metadata": {
        "id": "lDy2MXeaaGnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction of Energy Consumption of Home Appliances in a Low-Energy House"
      ],
      "metadata": {
        "id": "oiGuMSqWfjyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to work with the [appliances energy prediction](http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) dataset from the UCI Machine Learning Repository, associated to the paper by [Candanedo et al.](https://www.sciencedirect.com/science/article/pii/S0378778816308970). The objective of this dataset is to be able to predict the energy consumption of the home appliances in a low-energy house located in Belgium. The dataset contains Energy consumption from appliances at 10 minute resolution for about 4.5 months. The house temperature and humidity conditions were monitored with a wireless sensor network. Each wireless node transmitted the temperature and humidity conditions every 3.3 minutes. Then, the wireless data was averaged for 10-minute periods to match the energy consumption data. Additionally, weather data from the nearest airport weather station (Chievres Airport, Belgium) was logged.\n",
        "\n",
        "**The task is to predict the energy consumption based on the data gathered from the wireless sensor network and the weather data.** In the original paper, they were also focused on understanding which the most relevant features were. We will also work on it.\n",
        "\n",
        "> Although we are working with time-series data, in this case and following the original work, we will treat it as a standard regression dataset, aiming to predict the energy consumption of each point in time (every 10 minutes), just by the data taken at that same time frame."
      ],
      "metadata": {
        "id": "u1fIl_OXfuyF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9skDC04xbu35"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using this notebook in Google Colab, you will need to install Spark by running the following cell:"
      ],
      "metadata": {
        "id": "vGC9Y3M3iU3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "MBrdgWr6iaXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a106ca0b67125ec8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "oWd66GLhbu36"
      },
      "source": [
        "The first thing we need to do to start working with Spark is to initialize the `SparkSession`. We will also import a few libraries we will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5x4bbutbu36"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"MLlib Challenge\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAR7-iebbu37"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pyspark.sql.functions as sql_f # import SQL functions\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Helper function to test the correctness of the solutions\n",
        "def test(var, val, msg=\"\"):\n",
        "    print(\"Test passed.\") if var == val else print(\"Test failed. \" + msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and understand the data"
      ],
      "metadata": {
        "id": "1CPgcgu9jAlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download the dataset and save it as `energydata_complete.csv` by running the following cell:"
      ],
      "metadata": {
        "id": "HpTS4_XolJoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"energydata_complete.csv\" \"https://www.cs.nott.ac.uk/~pszit/data/energydata_complete.csv\""
      ],
      "metadata": {
        "id": "ySKTYKn_jCpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8937897c4473abfa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "lPAbO_Dabu37"
      },
      "source": [
        "Then we need to load the data, which is in Comma-Separated Value (CSV) format.\n",
        "\n",
        "**Task:** Use `spark.read` to read the file. You should also cache the data so that we only read it from disk once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3aadc724e217ea88",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wrsT4fr_bu38"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>\n",
        "\n",
        "# Cache your DataFrame\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-28b65c981c65a16d",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "O6IhsXj0bu38"
      },
      "outputs": [],
      "source": [
        "test(df.count(), 19735, 'Incorrect number of rows')\n",
        "test(df.is_cached, True, 'df not cached')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRtxruuibu39"
      },
      "source": [
        "**Task:** Use an appropriate method to check what columns we have in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c4b1a185e62db951",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OSBnpvZJbu39"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c8a8f77c243d18c0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tvF1USdubu39"
      },
      "source": [
        "**Data description**\n",
        "\n",
        "From the UCI repository description, we know that the columns have the following meanings:\n",
        "\n",
        "**Attribute information**:\n",
        "```\n",
        "date: time year-month-day hour:minute:second\n",
        "Appliances: energy use in Wh\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "T2: Temperature in living room area, in Celsius\n",
        "RH_2: Humidity in living room area, in %\n",
        "T3: Temperature in laundry room area\n",
        "RH_3: Humidity in laundry room area, in %\n",
        "T4: Temperature in office room, in Celsius\n",
        "RH_4: Humidity in office room, in %\n",
        "T5: Temperature in bathroom, in Celsius\n",
        "RH_5: Humidity in bathroom, in %\n",
        "T6: Temperature outside the building (north side), in Celsius\n",
        "RH_6: Humidity outside the building (north side), in %\n",
        "T7: Temperature in ironing room , in Celsius\n",
        "RH_7: Humidity in ironing room, in %\n",
        "T8: Temperature in teenager room 2, in Celsius\n",
        "RH_8: Humidity in teenager room 2, in %\n",
        "T9: Temperature in parents room, in Celsius\n",
        "RH_9: Humidity in parents room, in %\n",
        "To: Temperature outside (from Chievres weather station), in Celsius\n",
        "Pressure: (from Chievres weather station), in mm Hg\n",
        "RH_out: Humidity outside (from Chievres weather station), in %\n",
        "Wind speed: (from Chievres weather station), in m/s\n",
        "Visibility: (from Chievres weather station), in km\n",
        "Tdewpoint: (from Chievres weather station), Â°C\n",
        "rv1: Random variable 1, nondimensional\n",
        "rv2: Random variable 2, nondimensional\n",
        "```\n",
        "\n",
        "**The target variable is the energy use of the Appliances.**\n",
        "\n",
        "For now, we will leave the two variables `rv1` and `rv2` in our dataset, to see if they are affecting our methods much, then we can try to remove them and see if we improve the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5c3b6f3c781e12af",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Yi0J7B94bu3-"
      },
      "source": [
        "**Task:** Use `show` to visualize the data. Be careful not to show the entire data frame, only 5 rows!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0bc3c84df797cc37",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "tLLpFvNWbu3-"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-6a7113523db1a125",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ELCI5rjnbu3-"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-74870c89983edfb6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_-DFMYt0bu3-"
      },
      "source": [
        "This dataset is nicely prepared for Machine Learning and required very little preprocessing. However, rather than keeping the date as a timestamp, we would like to have some additional columns, including 'day of the year', 'hour', and 'month of the year'.\n",
        "\n",
        "**Task:** Use `.withColumn()` and other appropriate functions to add the three additional columns. Please use the naming `dayofyear`, `hour` and `month`, respectively.\n",
        "\n",
        "*Hint: Of course the SparkSQL library has functions to transform strings with datetime! You may find the [documentation](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html#functions) useful to explore to find specific functions.*\n",
        "\n",
        "Would you like to have any other information from the datetime? Feel free to add other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1f648a6d01eb2952",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ONz81koabu3-"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-97f185889dd05c76",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5pCtcG6wbu3-"
      },
      "outputs": [],
      "source": [
        "test(\"hour\" in df.columns, True, \"The hour hasn't been added\")\n",
        "test(\"dayofyear\" in df.columns, True, \"The dayofyear hasn't been added\")\n",
        "test(\"month\" in df.columns, True, \"The month hasn't been added\")\n",
        "test(df.select('dayofyear', 'hour', 'month', 'date').first(), (11, 17, 1, '2016-01-11 17:00:00'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2561535e29742ef2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7OrBNBODbu3-"
      },
      "source": [
        "**Task:** When your dataframe `df` has the additional columns, please remove the column `date`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-96bed972cec4c7a1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ihmcWyfabu3_"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-78b3ae64d5564833",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sInkAQDybu3_"
      },
      "outputs": [],
      "source": [
        "test(\"date\" in df.columns, False, \"Column date has not been removed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2e4f9de42e7d939c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5d_sxrrHbu3_"
      },
      "source": [
        "Let's look again at the schema of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f5ee1e564ac14dff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "tlLh2eSgbu3_"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-6bb2857862d64352",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "gCmCbXkvbu3_"
      },
      "source": [
        "Oh, no! All the input features have been inferred as strings rather than numeric values.\n",
        "\n",
        "**Task:** Transform that into numerical values. All of the features are actually numeric, so you could cast all of them. You are recommended to use functions like `cast` and `col` to do this. You could try to leave out the datetime columns we created, but it's fine if you transform them to float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f72f135db795ec70",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JquOK1Aybu3_"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-6291c14f0a17fcf0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "vNSozoCabu3_"
      },
      "outputs": [],
      "source": [
        "# Print the schema again.\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-0892fea0abdd7b6b",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SB9BI3oLbu3_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "for c in df.columns:\n",
        "    if c not in ['dayofyear', 'month', 'hour']:\n",
        "        test(df.schema[c].dataType, FloatType(), 'Incorrect data type for ' + c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c937a928bbbec3d7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IyFoO8pHbu3_"
      },
      "source": [
        "## Split data into training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a simple means to validate the results of the model we are going to build, we split the dataset into training and test sets. We will train and tune our model on the training set, and then see how well we do in the test.\n",
        "\n",
        "> Although in some time-series problems it is key to perform the appropriate train-test split considering the temporal component (e.g., [TimeSeriesSplit from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit), in our case we follow the procedure of the original paper where the interest lies in working with each data point independently, and hence, we can proceed with a standard random split."
      ],
      "metadata": {
        "id": "W8bmw-3XRv37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-557ed884643dbca0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "X35iPWyIbu3_"
      },
      "source": [
        "**Task:** Split the dataframe `df` into 70% for training (`df_train`) and 30% for test (`df_test`). You should fix a random seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-56849a52128319c8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7Wcl8STsbu3_"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1fb4a0da2280473a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "oHxicBKwbu4A"
      },
      "source": [
        "> Even though we have fixed the random seed, you will not always get the exact same split. Different computers with different operating systems may provide different splits with Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d4c519e050e792fd",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_MBPQHZTbu4A"
      },
      "source": [
        "Note that this is the simplest way of validating your results. You may want to carry out a [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) and split the dataset into *k* folds, and build and test *k* models. We will do later cross validation but for parameter tuning! Not to validate our approach!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-254b4625d65edbab",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uWf2mqKCbu4A"
      },
      "source": [
        "## Data visualisation\n",
        "\n",
        "Before applying any machine learning algorithm, it is a good practice to try to visualise your data. For example, we could see how much energy is spent in appliances depending on the month.\n",
        "\n",
        "We first group the dataframe by the `month` column, before summing together the values in the `Appliances` column for each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5521c415bf5963be",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "79hPOaG9bu4A"
      },
      "outputs": [],
      "source": [
        "# create a variable `hist_elect` that contains the histogram of total energy consumed by the Appliances, grouped by month\n",
        "hist_elect = df_train.groupBy('month').sum('Appliances').sort(\"month\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-2b25550471825981",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "EsKh8_JKbu4A"
      },
      "outputs": [],
      "source": [
        "(x_values, y_values) = zip(*hist_elect)\n",
        "plt.bar(x_values, y_values)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sum of Appliance Energy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-75f1197b0325c06d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "V2IpKAUHbu4A"
      },
      "source": [
        "Surprisingly, we don't seem to have all the data from the 1st of January as we have less consumption in that particular month.\n",
        "\n",
        "**Task:** Check the number of data points we have for each month. You will need to use `groupBy()` and an appropriate aggregation to get the number of data points for each group. You may wish to refer to the [Spark SQL Grouping documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2ed1b3f11fe51ec8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3EketqV4bu4A"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-e6dce63d19f9cabf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c0X7ttBYbu4A"
      },
      "source": [
        "We have fewer examples for both January and May. So probably the month is not a good feature, don't you think?\n",
        "\n",
        "Even if we did have the same number of examples for those months, it wouldn't make much sense to use it if we expect our model to predict future values in other months different than those. We are going to remove it from the DataFrame (and the training and test data frames too!).\n",
        "\n",
        "**Task:** Remove the `month` column from `df`, `df_train` and `df_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-fad68e0bbfd1e59f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "g_VNvmU3bu4E"
      },
      "outputs": [],
      "source": [
        "# update the variables `df`, `df_train` and `df_test`, removing the column 'month'\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-18a289e60bacafed",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "v62MA8DHbu4E"
      },
      "outputs": [],
      "source": [
        "test(\"month\" in df.columns, False, \"Column month has not been removed!\")\n",
        "test(\"month\" in df_train.columns, False, \"Column month has not been removed!\")\n",
        "test(\"month\" in df_test.columns, False, \"Column month has not been removed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d8aab95c80f20932",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BcoritW6bu4E"
      },
      "source": [
        "You could do other plots to understand better the data and practice with Spark. This is a good opportunity to practice with the DataFrame API!\n",
        "\n",
        "**Task:** Add any other analysis of the data you consider necessary in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-13f6aa1e22b192f1",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "t0oYZ02Pbu4E"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-e49995e14a31fa47",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "XP3stq2_bu4E"
      },
      "source": [
        "## Create a Pipeline with Spark ML\n",
        "\n",
        "As you know, we can't feed the DataFrame directly to a machine learning algorithm, as we need to put all the input features as an Array, and indicate which one is the output feature (in our case, the 'Appliances' column!).\n",
        "\n",
        "We will put together a simple Pipeline with the following stages:\n",
        "\n",
        "- VectorAssembler: To combine all the input columns into a single vector column (i.e., all the columns but the 'Appliances' one.\n",
        "- Learning algorithm: We feel like using Gradient-Boosted Trees [GBTs](https://en.wikipedia.org/wiki/Gradient_boosting) for this example, but feel free to use anything else.\n",
        "- CrossValidator: We will use cross validation to tune the parameters of the GBT model. Yes, this can be added as part of a pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3f24c59f54a6cf35",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "srJlt7DQbu4E"
      },
      "source": [
        "**Task:** Create the `VectorAssembler`. Do not forget to remove the target variable from the input columns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-706de5b087873dcc",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "CUm4peosbu4F"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-68a3d374f4a70c38",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Z80PBn6pbu4F"
      },
      "source": [
        "**Task:** Create an instance of `GBTRegressor` in which you don't indicate any parameters other than the target variable (i.e. `labelCol`), which should be set to `Appliances`. Assign it to a variable named `gbt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-8db3898a09503ca1",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Qqs-E5Wcbu4F"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-102cc339bfa52fe2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "H0q4vEDgbu4F"
      },
      "source": [
        "**Task:** Create a `CrossValidator` for `gbt`. You will first need to create a parameter grid and evaluator. The following cells guide you through these steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a46ae0c64349795f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "o_GuPg9cbu4F"
      },
      "source": [
        "You can explore the hyper-parameters you like for GBTs. Full documentation [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html). We would suggest to create a 'grid' for at least the depth of the tree and the number of iterations (e.g., we started with 10 and 20 iterations and the maximum depth at 5 and 8). If you are using your own computer, perhaps it is not a good idea to investigate more than 4-8 combinations of parameters.\n",
        "\n",
        "Use `ParamGridBuilder()` to create your parameter grid. Check the docs for how to use it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-54927b9e30d4d504",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6SrN1gX-bu4F"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-bc9e2db69dde356f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mw3tN__cbu4F"
      },
      "outputs": [],
      "source": [
        "# Define a variable `paramGrid` with some hyperparameters, e.g. for maxDepth, range [5,8], and for maxIter [10,20]\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4ed184ef97131b35",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PrF_Qc4Cbu4G"
      },
      "source": [
        "Create a `RegressionEvaluator`\n",
        " that uses the [Root Mean Squared Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) as our performance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a292d5c29d2d317c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0yz613Fsbu4G"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# create a variable `evaluator` with a rmse as metric for the prediction column given by `gbt`.\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-85da592a82153cc7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "C6gwZdD0bu4G"
      },
      "source": [
        "We can now create a CrossValidator `cv` that uses the `gbt` as estimator, as well as the evaluator and grid we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-ff22f5dbf35b6e2f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "VPA4BWKsbu4G"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2b33520df40a8ee9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zyljNDvWbu4G"
      },
      "source": [
        "Now we can put everything together into a Pipeline.\n",
        "\n",
        "**Task:** Create a Pipeline (`pipeline`) that contains the two stages, `vectorAssembler` and `cv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-c772c77b96ea0299",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "VbMIUzfLbu4G"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-83983f3ad8703821",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4Wd4UEOmbu4H"
      },
      "source": [
        "We are finally ready to fit our pipeline to the training data.\n",
        "\n",
        "**Task:** `fit` the model, and store in a `pipeline_model` variable. This might take quite a bit of time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-4865af895712af5e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JlEJjpPKbu4H"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-e1c48d0c5a11ba62",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_txv7bD2bu4H"
      },
      "source": [
        "**Task:** It could be a good idea to save the `pipelineModel` to disk, in case it takes too long, so we can read it later and re-use it. You should explore the [Pipeline API](https://spark.apache.org/docs/3.3.0/api/python/reference/api/pyspark.ml.PipelineModel.html#pyspark.ml.PipelineModel) to find out how to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-faf3b9bca3d42b86",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LqadSalfbu4H"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-77849593b37442d8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xSDfUTetbu4H"
      },
      "source": [
        "## Evaluate the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3c8a2cab62d704f5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "38ttgWtEbu4H"
      },
      "source": [
        "To obtain the predictions in the test set, apply the method `transform()` of the trained pipeline on the test DataFrame! This step will not apply the cross-validation, it will simply use the best model from the training stage.\n",
        "\n",
        "**Task:** Use `pipeline_model` to transform the test set, and store the resulting predictions in a variable called `predictions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d2a232d67c0b332b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a5IEOibKbu4H"
      },
      "outputs": [],
      "source": [
        "# create a variable `predictions`:\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a868d56e37754041",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZNUIxwNubu4H"
      },
      "source": [
        "It is easier to view the results when we limit the columns displayed to:\n",
        "\n",
        "- `Appliances`: the consumption of the Appliances in Wh\n",
        "- `prediction`: our predicted consumption\n",
        "\n",
        "**Task:** Find a way to show the output  with these two features only (and only for the first 5 rows):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-9846c3a76797db6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "evOpEhqDbu4I"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f37cc71440a8edf2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yofgqD3abu4I"
      },
      "source": [
        "Are these results any good? Let's compute the RMSE using the evaluator we created before!\n",
        "\n",
        "**Task:** Use `evaluator` to get the RMSE of the predictions from our `pipeline_model`. Store the result in a `rmse` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-1250797f2aae0195",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QWrEnUw3bu4I"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-7c9e770577bda460",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "UAshv4eqbu4I"
      },
      "outputs": [],
      "source": [
        "print(rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a6d64a5f7cdccf1c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "k5BKom0Abu4I"
      },
      "source": [
        "Seems a bit high?  Well, this number is relatively close to what it is reported in the original paper with RandomForest (RMSE around 69). But maybe you can investigate a bit more if you can improve that. Can we find out the importance of the features from the GBTs?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-98af8ec7cf0709ae",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "MMKLAaJObu4I"
      },
      "source": [
        "We first need to find out the best model!! In the way we trained the pipeline, you can find the trained model as one of the stages of the `pipeline_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a6849e473abdae3b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "15Tb0DJZbu4I"
      },
      "outputs": [],
      "source": [
        "cv_model = pipeline_model.stages[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-52f49ff72152882e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_y27pvaNbu4I"
      },
      "outputs": [],
      "source": [
        "cv_model.bestModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the feature importances for the `cv_model.bestModel` as shown in the following cell."
      ],
      "metadata": {
        "id": "DL2npr_4i2pL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-45a5bf04b6c73db5",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "q5bLvD00bu4J"
      },
      "outputs": [],
      "source": [
        "importance = cv_model.bestModel.featureImportances\n",
        "list(sorted(zip(featuresCols, importance.toArray()), key=lambda t: -t[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a4a6d02a351f7c19",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zxy_L21Lbu4J"
      },
      "source": [
        "Uhm, looks like our model gave the hour of the day (feature #28) quite a bit of importance. The random features ('rv1' and 'rv2', numbered, #25 and #26) were given different importance. While the model noticed that 'rv2' was completely useless, it gave some importance to 'rv1'.\n",
        "\n",
        "GBTs perform somehow an implicit feature selection, so those low-importance features won't affect much their performance, but we wonder if we could just remove low-importance features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-32a9d061f3e5a666",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UanAlz6mbu4J"
      },
      "source": [
        "## Removing low-importance features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-397368093280ee02",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vIgGDqZBbu4J"
      },
      "source": [
        "**Task:** Create a list of those features with importance less than 0.05. Your list should be called `to_remove`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-6b1e6b9006205fb3",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2yXymbmebu4J"
      },
      "outputs": [],
      "source": [
        "# create a list `to_remove` that contains the feature names that must be removed because their confidence is less than 0.05\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-76892a1a849b37d8",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "yHg7c4Nxbu4J"
      },
      "outputs": [],
      "source": [
        "print(to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c6802214fa240a6f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Q_Wo5LSqbu4J"
      },
      "source": [
        "Check the current schema of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-7eaf394a6020cdcd",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "XX7rjljUbu4J"
      },
      "outputs": [],
      "source": [
        "df_train.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8b82deb2dd980d57",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "erlojzvNbu4J"
      },
      "source": [
        "**Task:** Create a new VectorAssembler (called `vectorAssembler2`) with the low-importance columns removed.\n",
        "\n",
        "You don't need to remove the columns from the train and test dataframes as you won't be able to re-train the pipeline without modifying the `VectorAssembler`. You simply need to indicate the columns you want to use when creating a new `vectorAssembler2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-9452a5dfa044d94e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5gnveLedbu4J"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b545a033a4bed177",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "l5Ydrxhibu4K"
      },
      "source": [
        "**Task:** Create a new pipeline, `pipeline2`, with that new `vectorAssembler2`, and fit it to the training data. Use the resulting pipeline model to make predictions on the test set, and then compute the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d4a3e8633542f9e6",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5iJUTDZ7bu4K"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-5c708eda14fa475e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "TilsAngSbu4K"
      },
      "outputs": [],
      "source": [
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Check the features and feature importances of the best model."
      ],
      "metadata": {
        "id": "LZ5HRgVMlIC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-2b68661fd45b90f6",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kvffI6HVbu4K"
      },
      "outputs": [],
      "source": [
        "# Check the number of features used by the best model\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-c80ad5f678be804f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mD6ej6VPbu4L"
      },
      "outputs": [],
      "source": [
        "# Check the feature importances\n",
        "\n",
        "# <FILL-IN WITH YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d0777fc1ab7a64af",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "F6Srzp6Ubu4L"
      },
      "source": [
        "In this case, you will probably get less performance than before (will depend on your training and test partitions), but this time our model considered much fewer features! This might not have made our model more precise (because GBTs already ignored those features), but makes it more interpretable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a7c439bf5eedb412",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "33CZbhaJbu4L"
      },
      "source": [
        "# 2. Challenge: Improve the model further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ffeb040b4832b189",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ERIqDd0Zbu4L"
      },
      "source": [
        "You have now seen how to construct a simple pipeline using Spark MLlib, and begun to investigate the results you achieved with the `GBTRegressor`.\n",
        "\n",
        "There might be many ways to improve on the results we obtained here. We provide below some ideas for you to think about, categorised by whether they relate to the data preprocessing/preparation or the regression model itself. You are welcome to explore other options instead of our suggestions if you would like!\n",
        "\n",
        "**Data preparation**:\n",
        "- We haven't really done any careful pre-processing of the data. Are there outliers or noise that might be having an impact on the results?\n",
        "- Do we need any normalization?\n",
        "- You can still try with different subsets of features. We recommend you to take a look at the original paper and borrow some ideas to improve.\n",
        "\n",
        "**Regression model**:\n",
        "- Hyper-parameter tuning: We have used a relatively small set of parameters, and we haven't investigated what happened in training and test sets; Is there overfitting of the training set? Would we be able to use a larger number of trees?\n",
        "- The features of this dataset are numerical, are there other regression models that may be more appropriate than GBTs?\n",
        "\n",
        "### Minimum expectations for the challenge\n",
        "\n",
        "You should investigate *at least one* aspect from the data preparation stage and *at least one* aspect of the regression model for improving the results. You will need to implement the appropriate pipelines in Spark MLlib to achieve this.\n",
        "\n",
        "You will also need to evaluate the success of your new pipelines, and compare their results. In the tutorial above we only used RMSE, you can consider using additional metrics as well if you would like.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hF-Gj67Tnqos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vprMmCefpf39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7-A1LcFpfh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "name": "MLPipeline Bike Dataset",
    "notebookId": 3638908530782568,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}